{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping Code\n",
    "# https://realpython.com/python-web-scraping-practical-introduction/\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns true if the response seems to be HTML, false otherwise\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes to store extracted data\n",
    "leagueDF = pd.DataFrame(columns=['League_Name','League_Location','League_Country','League_ID', 'Status'])\n",
    "leagueAttendanceDF = pd.DataFrame(columns=['League_ID','League_Season','Year','League_Attendance', 'Date'])\n",
    "leagueCupDF = pd.DataFrame(columns=['League_ID','League_Cup','Year','Cup_Attendance', 'Date'])\n",
    "leagueChallengeDF = pd.DataFrame(columns=['League_ID','Challenge_Season','Year','Challenge_Attendance', 'Date'])\n",
    "\n",
    "# This method will download all of the League Challenge Data\n",
    "def Get_League_Challege(url, ID, page):\n",
    "    #Create URL\n",
    "    url = url + \"/challenge?&page_=\" + str(page)\n",
    "\n",
    "    #Search URL for HTML\n",
    "    raw_html = simple_get(url)\n",
    "    html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    \n",
    "    # Recursion\n",
    "    pages = html.find_all('a', attrs = {'class':'next'})#[1].get_text()[:4]\n",
    "    pageList = []\n",
    "    for index2 in range(0,len(pages)):\n",
    "        if pages[index2].get_text()[:4] == 'Next':\n",
    "            pageList.append('Next')\n",
    "    \n",
    "    if 'Next' in pageList:\n",
    "        Get_League_Challege(url, ID, (page+1))\n",
    "    \n",
    "    # league attendance table\n",
    "    row = html.find(\"tbody\").find_all(\"tr\")\n",
    "    \n",
    "    # Iterate through each row in the table\n",
    "    for index in range(0,len(row)):\n",
    "        cells = row[index].find_all(\"td\")\n",
    "        \n",
    "        # Get the data\n",
    "        challenge_Season = cells[0].get_text().replace('\\n','').replace('\\t','').strip()\n",
    "        year = cells[1].get_text()\n",
    "        challenge_Attendance = cells[2].get_text()\n",
    "        date = cells[3].get_text()\n",
    "        \n",
    "        # Save the record\n",
    "        leagueChallengeDF.loc[len(leagueAttendanceDF)] = [ID, challenge_Season, year, challenge_Attendance, date]\n",
    "\n",
    "# This method will download all of the League Attendance Data\n",
    "def Get_League_Attendance(url, ID, page):\n",
    "    #Create URL\n",
    "    url2 = url + \"/history?&page_=\" + str(page)\n",
    "\n",
    "    #Search URL for HTML\n",
    "    raw_html = simple_get(url2)\n",
    "    html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    \n",
    "    # Recursion\n",
    "    pages = html.find_all('a', attrs = {'class':'next'})#[1].get_text()[:4]\n",
    "    pageList = []\n",
    "    for index2 in range(0,len(pages)):\n",
    "        if pages[index2].get_text()[:4] == 'Next':\n",
    "            pageList.append('Next')\n",
    "    \n",
    "    if 'Next' in pageList:\n",
    "        Get_League_Attendance(url, ID, (page+1))\n",
    "    \n",
    "    # league attendance table\n",
    "    row = html.find_all('table')[0].find(\"tbody\").find_all(\"tr\")\n",
    "    \n",
    "    # Iterate through each row in the table\n",
    "    for index in range(0,len(row)):\n",
    "        cells = row[index].find_all(\"td\")\n",
    "        \n",
    "        # Get the data\n",
    "        league_Season = cells[0].get_text().replace('\\n','').replace('\\t','').strip()\n",
    "        year = cells[1].get_text()\n",
    "        league_Attendance = cells[2].get_text()\n",
    "        date = cells[3].get_text()\n",
    "        \n",
    "        # Save the record\n",
    "        leagueAttendanceDF.loc[len(leagueAttendanceDF)] = [ID, league_Season, year, league_Attendance, date]\n",
    "\n",
    "# This method will download all of the League Cup Data\n",
    "def Get_League_Cup(url, ID, page):\n",
    "    #Create URL\n",
    "    url = url + \"/cup?&page_=\" + str(page)\n",
    "\n",
    "    #Search URL for HTML\n",
    "    raw_html = simple_get(url)\n",
    "    html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    \n",
    "    # Recursion\n",
    "    pages = html.find_all('a', attrs = {'class':'next'})#[1].get_text()[:4]\n",
    "    pageList = []\n",
    "    for index2 in range(0,len(pages)):\n",
    "        if pages[index2].get_text()[:4] == 'Next':\n",
    "            pageList.append('Next')\n",
    "    \n",
    "    if 'Next' in pageList:\n",
    "        Get_League_Attendance(url, ID, (page+1))\n",
    "\n",
    "    # league cup table\n",
    "    row = html.find(\"tbody\").find_all(\"tr\")\n",
    "    #row = html.find_all('table')[0].find(\"tbody\").find_all(\"tr\")\n",
    "    \n",
    "    # Iterate through each row in the table\n",
    "    for index in range(0,len(row)):\n",
    "        cells = row[index].find_all(\"td\")\n",
    "    \n",
    "        # Get the data\n",
    "        cup_Season = cells[0].get_text().replace('\\n','').replace('\\t','').strip()\n",
    "        year = cells[1].get_text()\n",
    "        cup_Attendance = cells[2].get_text()\n",
    "        date = cells[3].get_text()\n",
    "        \n",
    "        # Save the record\n",
    "        leagueCupDF.loc[len(leagueCupDF)] = [ID, cup_Season, year, cup_Attendance, date]        \n",
    "\n",
    "# This method will download all of the League Data by calling the above methods\n",
    "def Get_League_Data(url, ID):\n",
    "    try:\n",
    "        #Search URL for HTML\n",
    "        raw_html = simple_get(url)\n",
    "        html = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "        # Get the League address\n",
    "        address_html = html.find_all('dl', attrs = {'class':'verysimpleDL2'})[0].find_all(\"dd\")\n",
    "        address = \"\"\n",
    "        for index in range(1,len(address_html)):\n",
    "            address = address + \" \" + address_html[index].get_text()    \n",
    "        address = address.replace(u'\\xa0', u'').strip()\n",
    "\n",
    "        # Get League's Country\n",
    "        league_Country = address_html[6].get_text()\n",
    "        # Get League's Name \n",
    "        league_Name = address_html[0].get_text()\n",
    "        \n",
    "        # Save Record\n",
    "        leagueDF.loc[len(leagueDF)] = [league_Name, address, league_Country, ID, 'Active']\n",
    "\n",
    "        # league attendance table\n",
    "        Get_League_Attendance(url,ID,1)\n",
    "        \n",
    "        # league challenge table\n",
    "        Get_League_Challege(url,ID,1)\n",
    "        \n",
    "        # league cup table\n",
    "        Get_League_Cup(url,ID,1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        # Try again\n",
    "        Get_League_Data(url, ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counter to print progress accross pages\n",
    "counter = 0\n",
    "\n",
    "# This method will search for each page of leagues, then searches for each league's data\n",
    "def Search_Webpage(url,page):\n",
    "    global counter\n",
    "\n",
    "    #Create URL\n",
    "    url2 = url + str(page)\n",
    "    \n",
    "    #Search URL for HTML\n",
    "    raw_html = simple_get(url2)\n",
    "    html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    \n",
    "    # Recursion\n",
    "    pages = html.find_all('a', attrs = {'class':'next'})\n",
    "    for index2 in range(0,len(pages)):\n",
    "        if pages[index2].get_text()[:4] == 'Next':\n",
    "            print('Page ' + str(page))\n",
    "            Search_Webpage(url, (page+1))\n",
    "    \n",
    "    # Search for table data\n",
    "    row = html.find_all('table', attrs = {'id':'table-1'})[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "    # Iterate each row in table\n",
    "    for index in range(0,len(row)):\n",
    "        # Get current row\n",
    "        cells = row[index].find_all(\"td\")\n",
    "\n",
    "        # get league Name\n",
    "        league_Name = cells[2].find_all('a', href=True)[0].get_text()\n",
    "\n",
    "        # get League ID\n",
    "        league_ID = cells[2].find_all('a', href=True)[0]['href'].split('/')[5]\n",
    "\n",
    "        # Use League ID to get League Data\n",
    "        League_URL = \"https://www.pokemon.com/uk/play-pokemon/pokemon-events/leagues/\" + str(league_ID)\n",
    "        \n",
    "        # Print Progress\n",
    "        counter = counter + 1\n",
    "        print(str(counter) + \" \" + league_Name)\n",
    "        \n",
    "        # Get the League's Data\n",
    "        Get_League_Data(League_URL, league_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the Web scraping\n",
    "Search_Webpage(\"https://www.pokemon.com/uk/play-pokemon/pokemon-events/find-an-event/?city=London&results_pp=100&location_name=&event_type=league&end_date=&event_name=&country=175&sort_order=distance&postal_code=&distance_within=99999&address=&product_type=tcg&start_date=0&state_other=&page_1=\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Results\n",
    "leagueAttendanceDF.to_csv(\"leagueAttendanceDF.csv\")\n",
    "leagueDF.to_csv(\"leagueDF.csv\")\n",
    "leagueCupDF.to_csv(\"leagueCupDF.csv\")\n",
    "leagueChallengeDF.to_csv(\"leagueChallengeDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
